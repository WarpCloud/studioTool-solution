
1.search_search_table_only.json   search_search 跨租户表文件互导

  wf内容：通过sql任务 创建es表，导入数据，导出数据，再通过脚本任务，执行distcp命令
          拷贝数据到目标端hdfs上面，再通过sql任务，在目标端创建es表，导入distcp 拷贝过来的内容导入到目标端 es表中
		  

2.search_search_table_many.json   search_search 跨租户多表文件互导

  wf内容：通过sql任务 创建多张es表，导入数据，导出数据，再通过脚本任务，执行distcp命令
          拷贝数据到目标端hdfs上面，再通过sql任务，在目标端创建es表，导入distcp 拷贝过来的内容导入到目标端 es表中


注意：
WF 参数解析：
${hostname_inceptor_A} 填写源端inceptor ip
${hostname_inceptor_B} 填写目标端inceptor ip
${hostname_distcp_A} 填写源端namenode hdfs ip
${hostname_distcp_B} 填写目标端namenode hdfs ip
${guardian_token} 填写源端inceptor guardian token

如果试用wf的脚本任务，但是同时需要调用hdfs等客户端，可以试用tdhclient，具体操作可详见workflow的操作文档，下面简单描述：
1 需要把TDH-client.tar 添加到/var/lib/workflow{id}/ 目录下
2 并重启workflow服务，才能source TDH-client 成功

只支持从安全到不开安全的distcp,不支持不开安全到开安全的，执行distcp 添加的参数：
-Dipc.client.fallback-to-simple-auth-allowed=true  这个参数不支持 源端和目标端都不开安全的环境，或者源端和目标端都开安全的环境
例如:hadoop distcp  -Dipc.client.fallback-to-simple-auth-allowed=true hdfs://node089:8020/sql_hbase hdfs://172.26.5.39:8020/test_data
	
